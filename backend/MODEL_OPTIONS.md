# Model Options & Fine-Tuning Discussion

## üéØ Current Setup

**What we're using:**
- **Base Model:** Stable Diffusion 1.5 (`runwayml/stable-diffusion-v1-5`)
- **ControlNet:** Canny Edge Detection (`lllyasviel/sd-controlnet-canny`)
- **Approach:** img2img with ControlNet guidance

**Current Issues:**
- Results might be too "generated" rather than subtle enhancement
- May not preserve original colors/textures well enough
- Could be creating new content instead of enhancing existing patterns

---

## ü§î What's Not Working?

Before deciding on changes, let's identify the specific issues:

1. **Too much generation?** (Creating new content instead of enhancing)
2. **Wrong style?** (Not minimal/line-art enough)
3. **Color preservation?** (Losing original colors/textures)
4. **Too strong?** (Enhancement is too obvious)
5. **Wrong ControlNet?** (Canny might not be the best for patterns)

---

## üîÑ Option 1: Change the Model

### A. Different Base Model

**Stable Diffusion XL (SDXL)**
- ‚úÖ Better quality, more detail
- ‚úÖ Better at following prompts
- ‚ùå Larger (~7GB), slower
- ‚ùå Might still be too "generated"

**Stable Diffusion 2.1**
- ‚úÖ Better prompt following than 1.5
- ‚úÖ Similar size to 1.5
- ‚ùå Still might generate too much

**Realistic Vision / DreamShaper**
- ‚úÖ Better at realistic images
- ‚úÖ Good for subtle enhancements
- ‚ùå Might need different ControlNet

### B. Different ControlNet

**Current: Canny Edge Detection**
- Detects edges, good for line art
- But might be too aggressive

**Alternatives:**

1. **ControlNet Lineart**
   - `lllyasviel/sd-controlnet-lineart`
   - Better for minimal line art
   - More subtle

2. **ControlNet Scribble**
   - `lllyasviel/sd-controlnet-scribble`
   - Good for freehand drawings
   - More natural

3. **ControlNet HED (Holistically-Nested Edge Detection)**
   - `lllyasviel/sd-controlnet-hed`
   - Softer edges
   - Better for natural patterns

4. **ControlNet OpenPose** (if doing figures)
   - For human/animal figures
   - Probably not needed here

**Recommendation:** Try **Lineart** or **HED** for more subtle results

---

## üé® Option 2: Fine-Tune Existing Model

### A. Prompt Engineering (Easiest)

**Current prompts might be too generic. We can:**

1. **Make prompts more specific:**
   ```
   "minimal line art, {description}, clean edges, 
   preserve original colors, subtle enhancement, 
   pattern-based, no new content, enhance existing only"
   ```

2. **Add negative prompts:**
   ```
   "blurry, distorted, low quality, artifacts, 
   new content, generated, artificial, 
   completely new image, different colors"
   ```

3. **Adjust parameters:**
   - Lower `strength` (0.15-0.25 instead of 0.3)
   - Lower `guidance_scale` (6.0-7.0 instead of 7.5)
   - More `num_inference_steps` (30-40 for better quality)

### B. Image Processing Pre/Post

**Before AI:**
- Better edge detection preprocessing
- Color preservation masks
- Pattern analysis

**After AI:**
- Blend with original (50/50 mix)
- Color correction to match original
- Edge-aware filtering

### C. Actual Fine-Tuning (Advanced)

**Train on your concept:**
- Collect examples of "subtle pattern enhancement"
- Fine-tune SD 1.5 on your dataset
- Requires GPU time and dataset

**LoRA (Low-Rank Adaptation):**
- Faster than full fine-tuning
- Train a small adapter
- Better for specific style

---

## üîß Option 3: Hybrid Approach

**Combine multiple techniques:**

1. **Use OpenCV for initial enhancement**
   - Edge detection
   - Pattern sharpening
   - Color preservation

2. **Then use AI for refinement**
   - Lower strength (0.15-0.20)
   - More subtle guidance

3. **Blend results**
   - 70% original + 30% AI enhanced
   - Preserve original colors

---

## üìä Quick Comparison

| Approach | Pros | Cons | Effort |
|----------|------|------|--------|
| **Change to Lineart ControlNet** | Subtle, minimal | Need to test | Low |
| **Lower strength + better prompts** | Quick fix | Might not be enough | Very Low |
| **Hybrid (OpenCV + AI)** | Best of both | More complex | Medium |
| **Fine-tune model** | Perfect results | Time, resources | High |
| **Change to SDXL** | Better quality | Slower, larger | Medium |

---

## üéØ My Recommendations (In Order)

### Quick Wins (Try First):

1. **Switch to Lineart ControlNet**
   ```python
   CONTROLNET_MODEL_ID = "lllyasviel/sd-controlnet-lineart"
   ```

2. **Lower enhancement strength**
   ```python
   DEFAULT_DENOISING_STRENGTH = 0.2  # Instead of 0.3
   ```

3. **Better prompts**
   - Add "preserve original colors"
   - Add "subtle enhancement only"
   - Stronger negative prompts

4. **More inference steps**
   ```python
   NUM_INFERENCE_STEPS = 30  # Instead of 20
   ```

### If That's Not Enough:

5. **Try HED ControlNet** (softer edges)
6. **Hybrid approach** (OpenCV + AI)
7. **Fine-tune** (if you have specific examples)

---

## üß™ Testing Strategy

1. **Create test cases:**
   - Same image, different settings
   - Compare results side-by-side

2. **A/B test:**
   - Current setup vs. new setup
   - User feedback on which is better

3. **Iterate:**
   - Start with quick wins
   - Move to more complex if needed

---

## üí¨ Questions to Answer

1. **What specifically is wrong?**
   - Too much generation?
   - Wrong style?
   - Colors wrong?

2. **What's your ideal outcome?**
   - Very subtle enhancement?
   - More obvious but still natural?
   - Specific style?

3. **Do you have example images?**
   - "Before" and "After" examples?
   - Reference images of desired style?

---

## üöÄ Next Steps

**Let's start with quick wins:**
1. Switch to Lineart ControlNet
2. Lower strength to 0.2
3. Improve prompts
4. Test and compare

**Then decide:**
- If better ‚Üí Great!
- If not ‚Üí Try HED or hybrid approach
- If still not ‚Üí Consider fine-tuning

---

**What do you think? What specific issues are you seeing with the current results?**

